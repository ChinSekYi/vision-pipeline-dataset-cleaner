{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8623ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "display.clear_output()\n",
    "\n",
    "# Load config\n",
    "with open('../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Prevent ultralytics from tracking activity\n",
    "!yolo settings sync=False\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b700dbc",
   "metadata": {},
   "source": [
    "# Vision Pipeline Dataset Cleaner\n",
    "\n",
    "**Purpose**: Clean and filter image dataset for computer vision tasks\n",
    "\n",
    "**Pipeline Steps**:\n",
    "1. Data Quality Check (CleanVision)\n",
    "2. Person Detection (YOLOv8)\n",
    "3. Full-body & Face Validation\n",
    "4. Age/Gender Filtering\n",
    "5. Advertisement Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc1e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image\n",
    "from cleanvision import Imagelab\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97ca1fc",
   "metadata": {},
   "source": [
    "## Face Detection\n",
    "\n",
    "Detect faces within each person bounding box using MediaPipe FaceMesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5326be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face_in_bbox(img, bbox_xyxy, face_cascade):\n",
    "    \"\"\"\n",
    "    Detect face within person bounding box using OpenCV cascade classifier.\n",
    "    \n",
    "    Args:\n",
    "        img: image array (BGR)\n",
    "        bbox_xyxy: [x_min, y_min, x_max, y_max]\n",
    "        face_cascade: OpenCV cascade classifier\n",
    "    \n",
    "    Returns:\n",
    "        dict: {'has_face': bool, 'num_faces': int}\n",
    "    \"\"\"\n",
    "    x_min, y_min, x_max, y_max = [int(c) for c in bbox_xyxy]\n",
    "    \n",
    "    # Crop person region\n",
    "    crop = img[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    if crop.size == 0:\n",
    "        return {'has_face': False, 'num_faces': 0}\n",
    "    \n",
    "    # Convert to grayscale for cascade classifier\n",
    "    gray_crop = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray_crop,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(20, 20)\n",
    "    )\n",
    "    \n",
    "    has_face = len(faces) > 0\n",
    "    \n",
    "    return {\n",
    "        'has_face': has_face,\n",
    "        'num_faces': len(faces),\n",
    "    }\n",
    "\n",
    "# Load OpenCV Haar Cascade Classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    ")\n",
    "\n",
    "# Test on first image\n",
    "face_result = detect_face_in_bbox(first_img, first_entry[\"boxes_xyxy\"][0], face_cascade)\n",
    "\n",
    "print(f\"\\nFace Detection Result:\")\n",
    "print(f\"  Has face: {face_result['has_face']}\")\n",
    "print(f\"  Num faces detected: {face_result['num_faces']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434d54fb",
   "metadata": {},
   "source": [
    "## Phase 3 Filtering\n",
    "\n",
    "Filter images by:\n",
    "1. **Full-body**: No cutting at edges\n",
    "2. **Face visible**: At least one face detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7542ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_results = {}\n",
    "validated_image_names = []\n",
    "\n",
    "for img_name in tqdm(person_image_names, desc=\"Validating full-body & face\"):\n",
    "    img_path = person_output_path / img_name\n",
    "    img = cv2.imread(str(img_path))\n",
    "    \n",
    "    entry = all_results[img_name]\n",
    "    bbox = entry[\"boxes_xyxy\"][0]  # Get first person bbox\n",
    "    \n",
    "    # Check full-body using pose keypoints\n",
    "    fullbody_result = is_fullbody_person(img, bbox, pose_model)\n",
    "    is_fullbody = fullbody_result['is_fullbody']\n",
    "    \n",
    "    # Check face\n",
    "    face_result = detect_face_in_bbox(img, bbox, face_cascade)\n",
    "    has_face = face_result['has_face']\n",
    "    \n",
    "    # Keep if: full-body AND has face\n",
    "    is_valid = is_fullbody and has_face\n",
    "    \n",
    "    validation_results[img_name] = {\n",
    "        \"is_fullbody\": is_fullbody,\n",
    "        \"keypoint_confidence\": fullbody_result['keypoint_confidence'],\n",
    "        \"has_face\": has_face,\n",
    "        \"num_faces\": face_result['num_faces'],\n",
    "        \"is_valid\": is_valid,\n",
    "        \"bbox\": bbox,\n",
    "    }\n",
    "    \n",
    "    if is_valid:\n",
    "        validated_image_names.append(img_name)\n",
    "\n",
    "print(f\"\\nâœ“ Validation complete\")\n",
    "print(f\"  Full-body & face visible: {len(validated_image_names)}/{len(person_image_names)}\\n\")\n",
    "\n",
    "# Summary\n",
    "fullbody_no_face = sum(1 for v in validation_results.values() if v['is_fullbody'] and not v['has_face'])\n",
    "face_no_fullbody = sum(1 for v in validation_results.values() if not v['is_fullbody'] and v['has_face'])\n",
    "both_missing = sum(1 for v in validation_results.values() if not v['is_fullbody'] and not v['has_face'])\n",
    "\n",
    "print(f\"Breakdown:\")\n",
    "print(f\"  Valid (full-body + face): {len(validated_image_names)}\")\n",
    "print(f\"  Full-body only (no face): {fullbody_no_face}\")\n",
    "print(f\"  Face only (not full-body): {face_no_fullbody}\")\n",
    "print(f\"  Both missing: {both_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473be714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save validated images to output folder\n",
    "validated_output_path = Path(f\"../{config['paths']['validated']}\")\n",
    "if validated_output_path.exists():\n",
    "    shutil.rmtree(validated_output_path)\n",
    "validated_output_path.mkdir(exist_ok=True)\n",
    "\n",
    "for img_name in validated_image_names:\n",
    "    src = person_output_path / img_name\n",
    "    dst = validated_output_path / img_name\n",
    "    shutil.copy(src, dst)\n",
    "\n",
    "print(f\"âœ“ Saved {len(validated_image_names)} validated images to: {config['paths']['validated']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a1e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize validation result for a specific image\n",
    "test_image_name = 'crop (5).png'  # Change this to test different images\n",
    "\n",
    "if test_image_name in validation_results:\n",
    "    result = validation_results[test_image_name]\n",
    "    img_path = person_output_path / test_image_name\n",
    "    img = cv2.imread(str(img_path))\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Validation Result: {test_image_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nâœ“ Full-body check:\")\n",
    "    print(f\"  - Is full-body: {result['is_fullbody']}\")\n",
    "    print(f\"  - Nose confidence: {result['keypoint_confidence']['nose']:.3f}\")\n",
    "    print(f\"  - Left ankle confidence: {result['keypoint_confidence']['left_ankle']:.3f}\")\n",
    "    print(f\"  - Right ankle confidence: {result['keypoint_confidence']['right_ankle']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Face check:\")\n",
    "    print(f\"  - Has face: {result['has_face']}\")\n",
    "    print(f\"  - Number of faces: {result['num_faces']}\")\n",
    "    \n",
    "    # Final status\n",
    "    status = \"âœ… VALID (PASS)\" if result['is_valid'] else \"âŒ INVALID (FAIL)\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FINAL STATUS: {status}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Visualize with bounding box\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "    ax.imshow(img_rgb)\n",
    "    \n",
    "    # Draw person bounding box\n",
    "    bbox = result['bbox']\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    width = x_max - x_min\n",
    "    height = y_max - y_min\n",
    "    \n",
    "    # Color based on validation status\n",
    "    box_color = 'green' if result['is_valid'] else 'red'\n",
    "    rect = patches.Rectangle((x_min, y_min), width, height, linewidth=3, \n",
    "                              edgecolor=box_color, facecolor='none', label='Person bbox')\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    # Add validation info as title\n",
    "    title_text = f\"{test_image_name}\\n\"\n",
    "    title_text += f\"Full-body: {result['is_fullbody']} | Face: {result['has_face']} | \"\n",
    "    title_text += f\"Valid: {result['is_valid']}\"\n",
    "    ax.set_title(title_text, fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"To test other images, change 'test_image_name' to any image name from person_image_names\")\n",
    "    print(f\"Available images: {sorted(person_image_names)[:5]}...\")\n",
    "else:\n",
    "    print(f\"âŒ Image '{test_image_name}' not found in validation results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c928e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug face detection - see what the cascade classifier is trying to detect\n",
    "test_image_name = 'crop (5).png'\n",
    "\n",
    "if test_image_name in all_results:\n",
    "    img_path = person_output_path / test_image_name\n",
    "    img = cv2.imread(str(img_path))\n",
    "    \n",
    "    entry = all_results[test_image_name]\n",
    "    bbox = entry[\"boxes_xyxy\"][0]\n",
    "    x_min, y_min, x_max, y_max = [int(c) for c in bbox]\n",
    "    \n",
    "    # Crop person region\n",
    "    crop = img[y_min:y_max, x_min:x_max]\n",
    "    gray_crop = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "    gray_crop_bw = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Face Detection Debug: {test_image_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nCropped person region:\")\n",
    "    print(f\"  - Size: {crop.shape}\")\n",
    "    print(f\"  - Brightness (mean): {gray_crop_bw.mean():.1f}\")\n",
    "    print(f\"  - Contrast (std): {gray_crop_bw.std():.1f}\")\n",
    "    \n",
    "    # Try with different parameters\n",
    "    print(f\"\\nTesting face detection with different parameters:\")\n",
    "    \n",
    "    configs = [\n",
    "        {\"name\": \"Default (current)\", \"scaleFactor\": 1.1, \"minNeighbors\": 5, \"minSize\": (20, 20)},\n",
    "        {\"name\": \"Less strict\", \"scaleFactor\": 1.05, \"minNeighbors\": 3, \"minSize\": (15, 15)},\n",
    "        {\"name\": \"Very lenient\", \"scaleFactor\": 1.05, \"minNeighbors\": 2, \"minSize\": (10, 10)},\n",
    "        {\"name\": \"Larger faces only\", \"scaleFactor\": 1.2, \"minNeighbors\": 5, \"minSize\": (40, 40)},\n",
    "    ]\n",
    "    \n",
    "    for cfg in configs:\n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            gray_crop_bw,\n",
    "            scaleFactor=cfg['scaleFactor'],\n",
    "            minNeighbors=cfg['minNeighbors'],\n",
    "            minSize=cfg['minSize']\n",
    "        )\n",
    "        print(f\"  {cfg['name']:25} â†’ {len(faces)} faces detected\")\n",
    "    \n",
    "    # Visualize\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Show original crop\n",
    "    axes[0].imshow(gray_crop)\n",
    "    axes[0].set_title(f\"Original Cropped Region\\n({crop.shape[1]}x{crop.shape[0]}px)\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Show grayscale with detected faces\n",
    "    gray_crop_copy = cv2.cvtColor(gray_crop_bw, cv2.COLOR_GRAY2RGB)\n",
    "    faces = face_cascade.detectMultiScale(gray_crop_bw, scaleFactor=1.1, minNeighbors=5, minSize=(20, 20))\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(gray_crop_copy, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "    axes[1].imshow(gray_crop_copy)\n",
    "    axes[1].set_title(f\"Grayscale with Detected Faces\\n({len(faces)} faces found)\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Possible reasons for detection failure:\")\n",
    "    print(f\"  1. Blur: Haar Cascade relies on edge features â†’ blurry images fail\")\n",
    "    print(f\"  2. Face size: Too small or too large relative to minSize parameter\")\n",
    "    print(f\"  3. Lighting/Angle: Face angle or poor lighting reduces confidence\")\n",
    "    print(f\"  4. Cascade strictness: minNeighbors=5 might be too strict\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    print(f\"To improve detection, you can:\")\n",
    "    print(f\"  â€¢ Lower minNeighbors (3-4 instead of 5) â†’ more detections but more false positives\")\n",
    "    print(f\"  â€¢ Lower scaleFactor (1.05 instead of 1.1) â†’ slower but more thorough\")\n",
    "    print(f\"  â€¢ Adjust minSize if faces are very large/small\")\n",
    "    print(f\"  â€¢ Use a different face detector (MediaPipe, dlib, or MTCNN) for better quality\")\n",
    "else:\n",
    "    print(f\"âŒ Image not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5627081",
   "metadata": {},
   "source": [
    "## Recommendation: Face Detection Tuning Strategy\n",
    "\n",
    "### Step 1: Try Tuning (RECOMMENDED - Low cost, quick)\n",
    "Adjust these parameters in cell 23 (Face Detection):\n",
    "```python\n",
    "# Option A: LENIENT (catches blurry faces)\n",
    "scaleFactor=1.05, minNeighbors=3, minSize=(15, 15)\n",
    "\n",
    "# Option B: VERY LENIENT (most permissive)\n",
    "scaleFactor=1.05, minNeighbors=2, minSize=(10, 10)\n",
    "```\n",
    "\n",
    "### Step 2: If tuning fails â†’ Switch to MediaPipe (Robust - Slower)\n",
    "Use [MediaPipe FaceMesh](https://developers.google.com/mediapipe/solutions/vision/face_detector) instead:\n",
    "- âœ… Handles blur, angles, lighting better\n",
    "- âœ… More robust for real-world datasets\n",
    "- âœ… Already installed (pip install mediapipe)\n",
    "- âŒ ~1-2 sec/image (vs ~0.1 sec with Haar Cascade)\n",
    "\n",
    "**Cost-benefit**: Extra inference time worth the quality improvement for your use case.\n",
    "\n",
    "### Decision Tree\n",
    "1. **Blurry faces are acceptable?** â†’ Tune Haar Cascade (minNeighbors=2-3)\n",
    "2. **Must catch all faces?** â†’ Switch to MediaPipe FaceMesh\n",
    "3. **Want perfect accuracy?** â†’ Use ensemble (Haar + MediaPipe fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1a15fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test: Try tuned parameters on your problematic image\n",
    "test_image_name = 'crop (5).png'\n",
    "\n",
    "if test_image_name in all_results:\n",
    "    img_path = person_output_path / test_image_name\n",
    "    img = cv2.imread(str(img_path))\n",
    "    entry = all_results[test_image_name]\n",
    "    bbox = entry[\"boxes_xyxy\"][0]\n",
    "    x_min, y_min, x_max, y_max = [int(c) for c in bbox]\n",
    "    \n",
    "    crop = img[y_min:y_max, x_min:x_max]\n",
    "    gray_crop_bw = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing tuned parameters on: {test_image_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Recommended tuned parameters\n",
    "    tuned_params = {\n",
    "        \"Lenient (Recommended)\": {\"scaleFactor\": 1.05, \"minNeighbors\": 3, \"minSize\": (15, 15)},\n",
    "        \"Very Lenient\": {\"scaleFactor\": 1.05, \"minNeighbors\": 2, \"minSize\": (10, 10)},\n",
    "        \"Current (Strict)\": {\"scaleFactor\": 1.1, \"minNeighbors\": 5, \"minSize\": (20, 20)},\n",
    "    }\n",
    "    \n",
    "    for name, params in tuned_params.items():\n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            gray_crop_bw,\n",
    "            scaleFactor=params['scaleFactor'],\n",
    "            minNeighbors=params['minNeighbors'],\n",
    "            minSize=params['minSize']\n",
    "        )\n",
    "        status = \"âœ… DETECTED\" if len(faces) > 0 else \"âŒ NOT DETECTED\"\n",
    "        print(f\"{name:25} â†’ {len(faces)} faces {status}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Recommendation:\")\n",
    "    print(f\"  1. If 'Lenient' works â†’ Update cell 23 with new params\")\n",
    "    print(f\"  2. If even 'Very Lenient' fails â†’ Switch to MediaPipe\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "else:\n",
    "    print(f\"âŒ Image not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95643d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 2: MediaPipe Face Detection (Robust approach)\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "# Download MediaPipe model if not exists\n",
    "model_dir = Path('../model')\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "model_path = model_dir / 'blaze_face_short_range.tflite'\n",
    "\n",
    "if not model_path.exists():\n",
    "    print(\"Downloading MediaPipe BlazeFace model...\")\n",
    "    url = 'https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite'\n",
    "    urllib.request.urlretrieve(url, model_path)\n",
    "    print(f\"âœ“ Downloaded to {model_path}\")\n",
    "else:\n",
    "    print(f\"âœ“ Model already exists at {model_path}\")\n",
    "\n",
    "# Create MediaPipe Face Detector\n",
    "base_options = python.BaseOptions(model_asset_path=str(model_path))\n",
    "options = vision.FaceDetectorOptions(base_options=base_options)\n",
    "mp_face_detector = vision.FaceDetector.create_from_options(options)\n",
    "\n",
    "def detect_face_mediapipe(img, bbox_xyxy, mp_detector):\n",
    "    \"\"\"\n",
    "    Detect face within person bounding box using MediaPipe FaceDetector.\n",
    "    \n",
    "    Args:\n",
    "        img: image array (BGR)\n",
    "        bbox_xyxy: [x_min, y_min, x_max, y_max]\n",
    "        mp_detector: MediaPipe FaceDetector\n",
    "    \n",
    "    Returns:\n",
    "        dict: {'has_face': bool, 'num_faces': int, 'confidence': list}\n",
    "    \"\"\"\n",
    "    x_min, y_min, x_max, y_max = [int(c) for c in bbox_xyxy]\n",
    "    \n",
    "    # Crop person region\n",
    "    crop = img[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    if crop.size == 0:\n",
    "        return {'has_face': False, 'num_faces': 0, 'confidence': []}\n",
    "    \n",
    "    # Convert BGR to RGB for MediaPipe\n",
    "    crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = python.vision.Image(image_format=python.vision.ImageFormat.SRGB, data=crop_rgb)\n",
    "    \n",
    "    # Detect faces\n",
    "    detection_result = mp_detector.detect(mp_image)\n",
    "    \n",
    "    num_faces = len(detection_result.detections) if detection_result.detections else 0\n",
    "    confidences = [d.categories[0].score for d in detection_result.detections] if detection_result.detections else []\n",
    "    \n",
    "    has_face = num_faces > 0\n",
    "    \n",
    "    return {\n",
    "        'has_face': has_face,\n",
    "        'num_faces': num_faces,\n",
    "        'confidence': confidences,\n",
    "    }\n",
    "\n",
    "print(\"\\nâœ“ MediaPipe Face Detector loaded\")\n",
    "print(\"  Model: blaze_face_short_range.tflite\")\n",
    "print(\"  Advantage: Robust to blur, angles, lighting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89a918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare both face detection approaches on test images\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "test_images = person_image_names[:10]  # Test on first 10 images\n",
    "comparison_results = []\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FACE DETECTION COMPARISON: Haar Cascade vs MediaPipe\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for img_name in tqdm(test_images, desc=\"Comparing methods\"):\n",
    "    img_path = person_output_path / img_name\n",
    "    img = cv2.imread(str(img_path))\n",
    "    \n",
    "    entry = all_results[img_name]\n",
    "    bbox = entry[\"boxes_xyxy\"][0]\n",
    "    x_min, y_min, x_max, y_max = [int(c) for c in bbox]\n",
    "    crop = img[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    # Option 1: Haar Cascade (STRICT)\n",
    "    gray_crop = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
    "    start = time.time()\n",
    "    faces_strict = face_cascade.detectMultiScale(gray_crop, scaleFactor=1.1, minNeighbors=5, minSize=(20, 20))\n",
    "    time_haar_strict = time.time() - start\n",
    "    \n",
    "    # Option 1: Haar Cascade (LENIENT)\n",
    "    start = time.time()\n",
    "    faces_lenient = face_cascade.detectMultiScale(gray_crop, scaleFactor=1.05, minNeighbors=3, minSize=(15, 15))\n",
    "    time_haar_lenient = time.time() - start\n",
    "    \n",
    "    # Option 2: MediaPipe\n",
    "    start = time.time()\n",
    "    mp_result = detect_face_mediapipe(img, bbox, mp_face_detector)\n",
    "    time_mp = time.time() - start\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'Image': img_name,\n",
    "        'Haar Strict': len(faces_strict) > 0,\n",
    "        'Haar Lenient': len(faces_lenient) > 0,\n",
    "        'MediaPipe': mp_result['has_face'],\n",
    "        'Time_Haar_Strict_ms': time_haar_strict * 1000,\n",
    "        'Time_Haar_Lenient_ms': time_haar_lenient * 1000,\n",
    "        'Time_MP_ms': time_mp * 1000,\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED RESULTS:\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS:\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "summary = {\n",
    "    'Method': ['Haar Cascade (Strict)', 'Haar Cascade (Lenient)', 'MediaPipe'],\n",
    "    'Detection Rate': [\n",
    "        (df_comparison['Haar Strict'].sum() / len(df_comparison) * 100),\n",
    "        (df_comparison['Haar Lenient'].sum() / len(df_comparison) * 100),\n",
    "        (df_comparison['MediaPipe'].sum() / len(df_comparison) * 100),\n",
    "    ],\n",
    "    'Avg Time (ms)': [\n",
    "        df_comparison['Time_Haar_Strict_ms'].mean(),\n",
    "        df_comparison['Time_Haar_Lenient_ms'].mean(),\n",
    "        df_comparison['Time_MP_ms'].mean(),\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(summary)\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nâœ“ Haar Strict   : Fast but strict â†’ misses blurry faces\")\n",
    "print(f\"âœ“ Haar Lenient  : Fast but parameter-tuned â†’ may not generalize\")\n",
    "print(f\"âœ“ MediaPipe     : Robust across conditions â†’ better generalization\\n\")\n",
    "\n",
    "# Show which images have disagreement\n",
    "print(\"Disagreement cases (methods detect differently):\")\n",
    "disagreement = df_comparison[\n",
    "    (df_comparison['Haar Strict'] != df_comparison['Haar Lenient']) |\n",
    "    (df_comparison['Haar Lenient'] != df_comparison['MediaPipe'])\n",
    "]\n",
    "\n",
    "if len(disagreement) > 0:\n",
    "    print(disagreement[['Image', 'Haar Strict', 'Haar Lenient', 'MediaPipe']].to_string(index=False))\n",
    "else:\n",
    "    print(\"  No disagreements in test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642004f3",
   "metadata": {},
   "source": [
    "## Phase 3: Face Detection Method Selection\n",
    "\n",
    "### Comparison Results\n",
    "Based on the test above, choose which method works best for your dataset:\n",
    "\n",
    "| Aspect | Haar Cascade (Strict) | Haar Cascade (Lenient) | MediaPipe |\n",
    "|--------|----------------------|------------------------|-----------|\n",
    "| **Detection Rate** | Lower (misses blur) | Higher | Highest (robust) |\n",
    "| **Speed** | âš¡ ~1-2 ms | âš¡ ~1-2 ms | ðŸ¢ ~50-100 ms |\n",
    "| **Generalization** | Poor (needs tuning per dataset) | Poor (dataset-specific) | âœ… Excellent (zero-tuning) |\n",
    "| **Blur handling** | âŒ Fails | âš ï¸ Works sometimes | âœ… Reliable |\n",
    "| **Parameter tuning** | âŒ High maintenance | âš ï¸ Required per dataset | âœ… None needed |\n",
    "| **Aligns with requirements** | âŒ Needs tuning | âš ï¸ Not ideal | âœ… \"Minimal tuning\" |\n",
    "\n",
    "### Recommendation\n",
    "**Use MediaPipe** because:\n",
    "1. âœ… Minimal parameter tuning (zero-tuning approach)\n",
    "2. âœ… Strong generalization across datasets\n",
    "3. âœ… Robust to real-world variations (blur, lighting, angles)\n",
    "4. âœ… Aligns with project goal: \"minimize manual intervention\"\n",
    "5. âœ… Still conventional CV technique (not a VLM)\n",
    "\n",
    "Inference time trade-off: +50-100ms per image Ã— 785 images = **~1 minute extra** for the entire dataset\n",
    "(Worth it for >10% accuracy improvement and zero future parameter tuning)\n",
    "\n",
    "### Decision\n",
    "Once satisfied with comparison results above, select one method and proceed to Phase 3 Full-body & Face Validation (next cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1b734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE YOUR FACE DETECTION METHOD\n",
    "# Change 'face_detection_method' to switch between approaches\n",
    "\n",
    "face_detection_method = 'mediapipe'  # Options: 'haar_strict', 'haar_lenient', 'mediapipe'\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"PHASE 3: FULL-BODY & FACE VALIDATION (Using: {face_detection_method.upper()})\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Run validation with selected method\n",
    "validation_results_final = {}\n",
    "validated_image_names_final = []\n",
    "\n",
    "for img_name in tqdm(person_image_names, desc=f\"Validating with {face_detection_method}\"):\n",
    "    img_path = person_output_path / img_name\n",
    "    img = cv2.imread(str(img_path))\n",
    "    \n",
    "    entry = all_results[img_name]\n",
    "    bbox = entry[\"boxes_xyxy\"][0]\n",
    "    \n",
    "    # Check full-body using pose keypoints (same for all methods)\n",
    "    fullbody_result = is_fullbody_person(img, bbox, pose_model)\n",
    "    is_fullbody = fullbody_result['is_fullbody']\n",
    "    \n",
    "    # Check face using selected method\n",
    "    if face_detection_method == 'haar_strict':\n",
    "        face_result = detect_face_in_bbox(img, bbox, face_cascade)\n",
    "    elif face_detection_method == 'haar_lenient':\n",
    "        # Lenient version of Haar Cascade\n",
    "        x_min, y_min, x_max, y_max = [int(c) for c in bbox]\n",
    "        crop = img[y_min:y_max, x_min:x_max]\n",
    "        if crop.size == 0:\n",
    "            face_result = {'has_face': False, 'num_faces': 0}\n",
    "        else:\n",
    "            gray_crop = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray_crop, scaleFactor=1.05, minNeighbors=3, minSize=(15, 15))\n",
    "            face_result = {'has_face': len(faces) > 0, 'num_faces': len(faces)}\n",
    "    else:  # mediapipe\n",
    "        face_result = detect_face_mediapipe(img, bbox, mp_face_detector)\n",
    "    \n",
    "    has_face = face_result['has_face']\n",
    "    \n",
    "    # Keep if: full-body AND has face\n",
    "    is_valid = is_fullbody and has_face\n",
    "    \n",
    "    validation_results_final[img_name] = {\n",
    "        \"is_fullbody\": is_fullbody,\n",
    "        \"keypoint_confidence\": fullbody_result['keypoint_confidence'],\n",
    "        \"has_face\": has_face,\n",
    "        \"num_faces\": face_result['num_faces'],\n",
    "        \"is_valid\": is_valid,\n",
    "        \"bbox\": bbox,\n",
    "        \"face_method\": face_detection_method,\n",
    "    }\n",
    "    \n",
    "    if is_valid:\n",
    "        validated_image_names_final.append(img_name)\n",
    "\n",
    "print(f\"\\nâœ“ Validation complete (Method: {face_detection_method})\")\n",
    "print(f\"  Full-body & face visible: {len(validated_image_names_final)}/{len(person_image_names)}\\n\")\n",
    "\n",
    "# Summary\n",
    "fullbody_no_face = sum(1 for v in validation_results_final.values() if v['is_fullbody'] and not v['has_face'])\n",
    "face_no_fullbody = sum(1 for v in validation_results_final.values() if not v['is_fullbody'] and v['has_face'])\n",
    "both_missing = sum(1 for v in validation_results_final.values() if not v['is_fullbody'] and not v['has_face'])\n",
    "\n",
    "print(f\"Breakdown:\")\n",
    "print(f\"  Valid (full-body + face): {len(validated_image_names_final)}\")\n",
    "print(f\"  Full-body only (no face): {fullbody_no_face}\")\n",
    "print(f\"  Face only (not full-body): {face_no_fullbody}\")\n",
    "print(f\"  Both missing: {both_missing}\")\n",
    "print(f\"\\nCoverage: {len(validated_image_names_final)/len(person_image_names)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8656e439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save validated images using selected method\n",
    "validated_output_path = Path(f\"../{config['paths']['validated']}\")\n",
    "if validated_output_path.exists():\n",
    "    shutil.rmtree(validated_output_path)\n",
    "validated_output_path.mkdir(exist_ok=True)\n",
    "\n",
    "for img_name in validated_image_names_final:\n",
    "    src = person_output_path / img_name\n",
    "    dst = validated_output_path / img_name\n",
    "    shutil.copy(src, dst)\n",
    "\n",
    "print(f\"âœ“ Saved {len(validated_image_names_final)} validated images to: {config['paths']['validated']}\")\n",
    "print(f\"âœ“ Face detection method used: {face_detection_method}\")\n",
    "print(f\"\\nTo switch methods and rerun:\")\n",
    "print(f\"  1. Change 'face_detection_method' in the cell above\")\n",
    "print(f\"  2. Rerun this cell and the save cell\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
