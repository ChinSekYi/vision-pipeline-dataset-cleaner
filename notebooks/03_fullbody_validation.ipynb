{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "display.clear_output()\n",
    "\n",
    "# Load config\n",
    "with open('../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Prevent ultralytics from tracking activity\n",
    "!yolo settings sync=False\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c9065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image\n",
    "from cleanvision import Imagelab\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49656524",
   "metadata": {},
   "source": [
    "# Phase 3 Full-body & Face Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fb4630",
   "metadata": {},
   "source": [
    "Info: if a person is \"cut off,\" their bounding box will touch the very edge of the image frame.\n",
    "Check the $y_{min}$ (top) and $y_{max}$ (bottom) of the bounding box. If the top of the box is at pixel 0, the head is likely cut off. If the bottom is at the maximum image height, the feet are cut off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daea24f",
   "metadata": {},
   "source": [
    "# Todo\n",
    "Key design decision: use bbox to check for full body image instead of another model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a39fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f1e75f",
   "metadata": {},
   "source": [
    "## Full-body Detection: Method Comparison & Design Decision\n",
    "\n",
    "### Problem Statement\n",
    "Dataset requirement: **\"Full-body person crops only (No feet or hands are acceptable)\"**\n",
    "\n",
    "How do we validate that a person is truly full-body and not cut off?\n",
    "\n",
    "### Method A: Geometry-based Bounding Box Check\n",
    "**Logic**: If a person is \"cut off,\" their bounding box will touch the very edge of the image.\n",
    "- Check if `x_min < margin` or `x_max > (width - margin)` → horizontally cut off\n",
    "- Check if `y_min < margin` or `y_max > (height - margin)` → vertically cut off (head/feet missing)\n",
    "- **Pros**: Fast, simple, no extra inference needed\n",
    "- **Cons**: Unreliable with loose bboxes; hand/feet could be in frame but still reject valid images\n",
    "\n",
    "### Method B: Pose Keypoint Validation ✓ **CHOSEN**\n",
    "**Logic**: Use YOLOv8-Pose to detect actual body keypoints (nose, ankles). A full-body person must have:\n",
    "- ✅ Nose visible (head is visible)\n",
    "- ✅ Left ankle visible (left leg visible)\n",
    "- ✅ Right ankle visible (right leg visible)\n",
    "- **Pros**: Detects actual body parts, not bbox edges; more robust to bbox variations\n",
    "- **Cons**: Slower (additional pose inference per image)\n",
    "\n",
    "### Why Method B?\n",
    "1. **Accuracy**: Your dataset requirement is strict → must validate actual body parts, not just box position\n",
    "2. **Robustness**: YOLOv8 bbox can be loose or tight; Method A would fail on loose bboxes\n",
    "3. **Quality Guarantee**: Ensures only truly full-body images pass through\n",
    "4. **Trade-off Worth It**: ~2-3x slower inference, but significantly higher confidence in output quality\n",
    "\n",
    "### Validation Rule\n",
    "```\n",
    "is_fullbody = (\n",
    "    confidence(nose) > 0.5 AND \n",
    "    confidence(left_ankle) > 0.5 AND \n",
    "    confidence(right_ankle) > 0.5\n",
    ")\n",
    "```\n",
    "\n",
    "**Expected Result**: Stricter filter, fewer false positives, higher dataset quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3f230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv8-Pose model for keypoint detection\n",
    "pose_model = YOLO(f\"../model/yolov8n-pose.pt\")\n",
    "\n",
    "def is_fullbody_person(img, bbox_xyxy, pose_model, keypoint_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Check if person has full body visible using pose keypoints.\n",
    "    \n",
    "    Args:\n",
    "        img: image array (BGR)\n",
    "        bbox_xyxy: [x_min, y_min, x_max, y_max]\n",
    "        pose_model: YOLOv8-Pose model\n",
    "        keypoint_threshold: confidence threshold for keypoints (default: 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        dict: {'is_fullbody': bool, 'keypoint_confidence': dict}\n",
    "    \"\"\"\n",
    "    # Run pose detection on full image\n",
    "    results = pose_model.predict(source=img, verbose=False)\n",
    "    result = results[0]\n",
    "    \n",
    "    if not result.keypoints or len(result.keypoints) == 0:\n",
    "        return {'is_fullbody': False, 'keypoint_confidence': {}}\n",
    "    \n",
    "    # Get keypoints for first person (should be our detected person)\n",
    "    keypoints = result.keypoints[0]\n",
    "    \n",
    "    # COCO keypoint indices: 0=nose, 15=left_ankle, 16=right_ankle\n",
    "    nose_conf = keypoints.conf[0, 0].item() if keypoints.conf is not None else 0\n",
    "    left_ankle_conf = keypoints.conf[0, 15].item() if keypoints.conf is not None else 0\n",
    "    right_ankle_conf = keypoints.conf[0, 16].item() if keypoints.conf is not None else 0\n",
    "    \n",
    "    # Full-body: must have nose + both ankles with high confidence\n",
    "    is_fullbody = (\n",
    "        nose_conf > keypoint_threshold and \n",
    "        left_ankle_conf > keypoint_threshold and \n",
    "        right_ankle_conf > keypoint_threshold\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'is_fullbody': is_fullbody,\n",
    "        'keypoint_confidence': {\n",
    "            'nose': float(nose_conf),\n",
    "            'left_ankle': float(left_ankle_conf),\n",
    "            'right_ankle': float(right_ankle_conf),\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b349811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first person image for testing\n",
    "first_img_name = person_image_names[0]\n",
    "first_img_path = person_output_path / first_img_name\n",
    "first_img = cv2.imread(str(first_img_path))\n",
    "first_entry = all_results[first_img_name]\n",
    "\n",
    "print(f\"Loaded first image for testing: {first_img_name}\")\n",
    "print(f\"Image shape: {first_img.shape}\")\n",
    "print(f\"Persons in image: {len(first_entry['boxes_xyxy'])}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
